{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KegaPlayer/kegaiengine/blob/main/Koboldcpp_Colab_(Another_Edited_Edition).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this one: https://colab.research.google.com/github/kalomaze/koboldcpp/blob/alternate_colab/Koboldcpp_Colab_(Improved_Edition).ipynb\n",
        "\n",
        "Original Koboldcpp Colab: https://colab.research.google.com/github/LostRuins/koboldcpp/blob/concedo/colab.ipynb\n",
        "\n",
        "---\n",
        "*Edited koboldcpp colab notebook for the KegAIEngine project.* Experiment to your heart's content, just as I may have done with this before this 'public' release.\n",
        "\n",
        "*   Easy and fast installing provided by the original Colab\n",
        "*   Can display the model's name inside the UI when choosing from the dropdown.\n",
        "*   Allows to set up a Kobold Horde worker if you feel like sharing Google's GPU compute power to the world\n",
        "*   A custom selection of models to choose from. The models available are curated under the maker's discretion or via suggestions...\n",
        "*   Ability to keep track of the models used by the user on a private spreadsheet\n",
        "\n",
        "## Credits\n",
        "- Made with ~~spite~~ love by kalomaze ❤️ <sub>\n",
        " - (also here's the part where I (kalomaze) shill my [Patreon](https://www.patreon.com/kalomaze) if you care!)</sub>\n",
        "- Edited and (sorta) updated with a bit of spite by KegaPlayer. Not owner of a Patreon but can be contacted at:\n",
        " * Discord: [kegaplayer](https://lookup.guru/183692020131299339)\n",
        " * Twitter: [KegaMPlayer](https://twitter.com/KegaMPlayer)\n",
        " * Mail: <potatokaigen@gmail.com>\n",
        "\n",
        "### Koboldcpp is not a software originally made by the aforementioned users. This notebook is just to make it easy to use on Colab, for research use and beyond. You can find the original GitHub repository for it here: https://github.com/LostRuins/koboldcpp"
      ],
      "metadata": {
        "id": "VhbuN9Yf6bc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "outputs": [],
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from google.oauth2.service_account import Credentials\n",
        "import hashlib\n",
        "import gspread\n",
        "\n",
        "#@title # **Koboldcpp Colab (Another Edited Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "#Koboldcpp now can autorun itself without building, yay! Acnowledged by the original dev also!\n",
        "\n",
        "Model_C = \"Emerhyst-20B\" #@param [\"Kunoichi-DPO-v2-7B\", \"UNA-TheBeagle-7B-v1\", \"Fimbulvetr-11B-v2\", \"Athena-v4-13B\", \"Mythalion-13B\", \"Xwin-MLewd-13B-v0.2\", \"Emerhyst-20B\", \"Rose-Kimiko-20B\"]\n",
        "Model = Model_C\n",
        "Quant_Method = \"3_K_S\" #@param [\"2_K\", \"3_K_S\", \"3_K_M\", \"3_K_L\", \"4_0\", \"4_K_S\", \"4_K_M\", \"5_0\", \"5_K_S\", \"5_K_M\", \"6_K\", \"8_0\"]\n",
        "\n",
        "#@markdown #### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "#@markdown (Example of a manual link: https://huggingface.co/TheBloke/Emerhyst-20B-GGUF/resolve/main/emerhyst-20b.Q2_K.gguf)\n",
        "#@markdown #### OPTIONAL: Use LoRA\n",
        "Use_Lora = False #@param {type:\"boolean\"}\n",
        "Lora_Link = \"\" #@param {type:\"string\"}\n",
        "#@markdown (Same format applied as Manual Link)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Launch Options\n",
        "\n",
        "Layers = \"65\" # @param [\"35\", \"43\", \"65\"] {allow-input: true}\n",
        "#@markdown (35 are all layers for 7B models. 43 are all layers for 13B models. 65 are all layers for 20B models)\n",
        "Context = \"4096\" #@param [\"512\",\"1024\",\"2048\",\"3072\",\"4096\",\"6144\",\"8192\",\"12288\",\"16384\"]{allow-input: true}\n",
        "Smart_Context = False #@param {type:\"boolean\"}\n",
        "#@markdown (The default (and recommended) setting is 4096. Use higher sizes with caution, lower sizes are NOT recommended. Use Smart Context at your own risk.)\n",
        "ForceRebuild = False #@param {type:\"boolean\"}\n",
        "#@markdown (Builds the Latest Kobold version. Can take ~7 minutes)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# @markdown # Setup Horde Worker\n",
        "\n",
        "# @markdown (Available for experimental gimmick reasons. The maker may not be held responsible if you end up banned off the service from using this)\n",
        "Enable_Horde_Worker = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Name of the model that's going to be used. For Manual Mode only\n",
        "Horde_Model_Name = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Sets how long (in tokens) will be the replies served\n",
        "Horde_Gen_Length = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown Necessary to make the worker available for use. [Get a key here](https://aihorde.net/register)\n",
        "Horde_API_Key = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown How the worker will be named. Leaving this empty will default to the name (KegAIEngine) and a random identifier\n",
        "Horde_Worker_Name = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#For horde\n",
        "horde_params = set()\n",
        "if Enable_Horde_Worker:\n",
        "  horde_params.add(Horde_Gen_Length)\n",
        "  horde_params.add(Context)\n",
        "\n",
        "  if Use_Manual_Model:\n",
        "    if Horde_Model_Name.strip():\n",
        "      print(f\"\\nManual Model detected; Using the name provided by the user: {hordemodelname}\")\n",
        "    else:\n",
        "      print(f\"\\nManual Model detected but model name not provided, falling back to default name used\\n\")\n",
        "      Horde_Model_Name = \"concedo\"\n",
        "  else:\n",
        "    Horde_Model_Name = Model_C\n",
        "\n",
        "  if Horde_Worker_Name.strip():\n",
        "    #Name detected\n",
        "    print(\"\\nWorker Name detected and now being used\")\n",
        "  else:\n",
        "    #Name is empty\n",
        "    print(\"\\nWorker Name not provided. Using default settings\")\n",
        "    import random\n",
        "    import string\n",
        "    RndID = ''.join(random.choice(string.ascii_uppercase) for i in range(3))\n",
        "    Horde_Worker_Name = \"KegAIEngine-{}\".format(RndID)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Analytics\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "# Updates the spreadsheet with the stats of the model when ran\n",
        "def update_llama_stats(DownloadedModel_path):\n",
        "    # Initialize gspread\n",
        "    scope = [\n",
        "        'https://www.googleapis.com/auth/spreadsheets',\n",
        "        'https://www.googleapis.com/auth/drive.file',\n",
        "        'https://www.googleapis.com/auth/drive'\n",
        "    ]\n",
        "\n",
        "    os.makedirs(\"/content/koboldcpp/stats/\", exist_ok=True)\n",
        "    !wget -q https://cdn.discordapp.com/attachments/945486970883285045/1114717554481569802/peppy-generator-388800-07722f17a188.json -O /content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json\n",
        "    config_path = '/content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json'\n",
        "\n",
        "    if os.path.exists(config_path):\n",
        "        # File exists, proceed with creation of creds and client\n",
        "        creds = Credentials.from_service_account_file(config_path, scopes=scope)\n",
        "        client = gspread.authorize(creds)\n",
        "    else:\n",
        "        # File does not exist, print message and skip creation of creds and client\n",
        "        print(\"Sheet credential file missing.\")\n",
        "        exit()  # Exit the script if the credentials are missing\n",
        "\n",
        "    # Open the Google Sheet\n",
        "    book = client.open(\"LlamaStats\")\n",
        "    sheet = book.get_worksheet(0)  # get the first sheet\n",
        "\n",
        "    DownloadedModel_name = os.path.basename(DownloadedModel_path)\n",
        "    DownloadedModel_hash = calculate_md5(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "    colA_values = sheet.col_values(1)\n",
        "    colB_values = sheet.col_values(2)\n",
        "    colC_values = sheet.col_values(3)\n",
        "\n",
        "    update_idx = -1\n",
        "\n",
        "    for idx in range(len(colA_values)):\n",
        "        if colA_values[idx] == DownloadedModel_name and idx < len(colB_values) and colB_values[idx] == DownloadedModel_hash:\n",
        "            update_idx = idx + 1\n",
        "            break\n",
        "\n",
        "    if update_idx == -1:\n",
        "        update_idx = len(colA_values) + 1\n",
        "\n",
        "    current_count = colC_values[update_idx - 1] if update_idx <= len(colC_values) else ''\n",
        "    if current_count.isdigit():\n",
        "        new_count = str(int(current_count) + 1)\n",
        "    else:\n",
        "        new_count = '1'\n",
        "\n",
        "    # Batch update to Google Sheets\n",
        "    cell_list = [\n",
        "        gspread.models.Cell(update_idx, 1, DownloadedModel_name),\n",
        "        gspread.models.Cell(update_idx, 2, DownloadedModel_hash),\n",
        "        gspread.models.Cell(update_idx, 3, new_count),\n",
        "        gspread.models.Cell(update_idx, 4, DownloadedModel_path)\n",
        "    ]\n",
        "    sheet.update_cells(cell_list)\n",
        "    print(\"\\nUpdating values...\\n\")\n",
        "\n",
        "#@markdown ##### OPTIONAL: Submit Download stats (for measuring model usage/popularity)\n",
        "Submit_Download_Stats = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_links = {\n",
        "    \"Kunoichi-DPO-v2-7B\": \"https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/resolve/main/kunoichi-dpo-v2-7b.Q2_K.gguf\",\n",
        "    \"UNA-TheBeagle-7B-v1\": \"https://huggingface.co/TheBloke/UNA-TheBeagle-7B-v1-GGUF/resolve/main/una-thebeagle-7b-v1.Q2_K.gguf\",\n",
        "    \"Fimbulvetr-11B-v2\": \"https://huggingface.co/Sao10K/Fimbulvetr-11B-v2-GGUF/resolve/main/Fimbulvetr-11B-v2-Test-14.q4_K_M.gguf\",\n",
        "    \"Athena-v4-13B\": \"https://huggingface.co/TheBloke/Athena-v4-GGUF/resolve/main/athena-v4.Q{}.gguf\",\n",
        "    \"Mythalion-13B\": \"https://huggingface.co/TheBloke/Mythalion-13B-GGUF/resolve/main/mythalion-13b.Q{}.gguf\",\n",		
        "    \"Xwin-MLewd-13B-v0.2\": \"https://huggingface.co/TheBloke/Xwin-MLewd-13B-v0.2-GGUF/blob/main/xwin-mlewd-13b-v0.2.Q{}.gguf\",\n",
        "    \"Emerhyst-20B\": \"https://huggingface.co/TheBloke/Emerhyst-20B-GGUF/resolve/main/emerhyst-20b.Q{}.gguf\",\n",
        "    \"Rose-Kimiko-20B\": \"https://huggingface.co/TheBloke/Rose-Kimiko-20B-GGUF/resolve/main/rose-kimiko-20b.Q{}.gguf\",\n",
        "}\n",
        "\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    # Use aria2c to download\n",
        "    print(\"Installing/updating aria2c...\")\n",
        "    !apt-get install aria2 -y >/dev/null 2>&1\n",
        "    print(\"Finished installing aria2c.\")\n",
        "\n",
        "    os.makedirs('/content/koboldcpp/', exist_ok=True)\n",
        "    if Use_Lora:\n",
        "      if Lora_Link.strip():\n",
        "          # Lora is enabled & link provided\n",
        "          print(\"\\nLora detected, will apply to model.\\n\")\n",
        "          lora = Lora_Link.replace('/blob/', '/resolve/')\n",
        "      else:\n",
        "          # Lora is enabled but no link\n",
        "          print(\"\\nWarning: Lora enabled, but no link, not applying.\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Manual_Link.strip():\n",
        "            # Manual Model is enabled, and a link is provided\n",
        "            print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "            Model = Manual_Link.replace('/blob/', '/resolve/')\n",
        "        else:\n",
        "            # Manual Model is enabled, but no link is provided\n",
        "            print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "            if Model in model_links:\n",
        "                Model = model_links[Model].format(Quant_Method)\n",
        "    else:\n",
        "        # Model is in model_links and has a supported format\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "    if not re.search(r'(\\.gguf|\\.ggml|\\.bin|\\.safetensors)$', Model):\n",
        "        print(\"--------------------------\\n5 SECOND WARNING: Manual link provided doesn't end with a supported format.\\nAre you sure you provided a direct link?\\n--------------------------\\n\")\n",
        "        time.sleep(5)\n",
        "    elif Model.startswith('https://huggingface.co/') and not re.search(r'^https://huggingface\\.co/.+/.+/.+/.+/[^/]+\\.[^/]+$', Model):\n",
        "        print(\"--------------------------\\n10 SECOND WARNING: The HuggingFace link provided is of the entire model repository.\\nPlease find the direct link to the quant you want to use.\\n--------------------------\\n\")\n",
        "        time.sleep(10)\n",
        "\n",
        "def download_model_and_lora():\n",
        "    if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n--------------------------\\nDownloading {os.path.basename(Model)}...\")\n",
        "        os.chdir(\"/content/koboldcpp\")\n",
        "        !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        elapsed_time = time.time() - start_time # Calculate and display elapsed time\n",
        "        print(f\"\\nDownload took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        if Use_Lora:\n",
        "          print(f\"\\n--------------------------\\nDownloading {os.path.basename(lora)}...\")\n",
        "          os.chdir(\"/content/koboldcpp\")\n",
        "          !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o lora.bin 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/model.gguf') and os.path.getsize(\"/content/koboldcpp/model.gguf\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/lora.bin') and os.path.getsize(\"/content/koboldcpp/lora.bin\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/lora.bin\")\n",
        "\n",
        "        if Submit_Download_Stats and os.path.exists(\"/content/koboldcpp/model.gguf\"):\n",
        "            DownloadedModel = Model[:]  # DownloadedModel is used for download stats\n",
        "            update_llama_stats(DownloadedModel)\n",
        "\n",
        "        print(\"--------------------------\\n\")\n",
        "    else:\n",
        "         print(\"--------------------------\\nModel already downloaded; skipping redownload.\\nDisconnect and delete runtime if you need to restart the colab fully.\\n--------------------------\\n\")\n",
        "\n",
        "thread = threading.Thread(target=download_model_and_lora)\n",
        "\n",        
        "# Checking if you already have a Kobold install\n",
        "if not os.path.exists(\"/content/koboldcpp/llama.cpp\"):\n",
        "    print(\"Downloading & extracting prebuilt Koboldcpp...\")\n",
        "\n",
        "    %cd /content\n",
        "    !git clone https://github.com/LostRuins/koboldcpp\n",
        "    %cd /content/koboldcpp\n",
        "    kvers = !(cat koboldcpp.py | grep 'KcppVersion = ' | cut -d '\"' -f2)\n",
        "    kvers = kvers[0]\n",
        "    if ForceRebuild:\n",
        "      kvers = \"force_rebuild\"\n",
        "    !echo Finding prebuilt binary for {kvers}\n",
        "    !wget -O dlfile.tmp https://kcppcolab.concedo.workers.dev/?{kvers} && mv dlfile.tmp koboldcpp_cublas.so\n",
        "    !test -f koboldcpp_cublas.so && echo Prebuilt Binary Exists || echo Prebuilt Binary Does Not Exist\n",
        "    !test -f koboldcpp_cublas.so && echo Build Skipped || make koboldcpp_cublas LLAMA_CUBLAS=1 LLAMA_PORTABLE=1\n",
        "    !cp koboldcpp_cublas.so koboldcpp_cublas.dat\n",
        "\n",
        "    thread.start()\n",
        "\n",
        "    print(\"\\nKobold extraction to /content/koboldcpp/ completed!\")\n",
        "    print(\"--------------------------\\n\")\n",
        "else:\n",
        "    # In case koboldcpp already exists, just start the model download\n",
        "    thread.start()\n",
        "\n",
        "thread.join()\n",
        "\n",
        "#Running localtunnel server\n",
        "!npm install -g localtunnel\n",
        "!nohup lt --port 5001 &\n",
        "\n",
        "# Check nohup.out for \"protocol=quic\" which signifies it launched\n",
        "!cat nohup.out\n",
        "print(\"--------------------------\\n\")\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "def hordeworkerenabled():\n",
        "  if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "  elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "  else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n",
        "\n",
        "def runkbcpp():\n",
        "  if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Smart_Context:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "        else:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "    else:\n",
        "        if Smart_Context:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "        else:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "  elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --remotetunnel --onready \"curl -s ipecho.net/plain | xargs -I % echo Scroll down and locate your Localtunnel link, For Localtunnel use the IP provided to verify: % && cat nohup.out\"\n",
        "  else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n",
        "\n",
        "if Enable_Horde_Worker:\n",
        "  hordeworkerenabled()\n",
        "else:\n",
        "  runkbcpp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQl3v9wali-"
      },
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "<sub>Note that some of the images may be outdated. But the overall function of the Colab is still the same!</sub>\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in [SillyTavern](https://docs.sillytavern.app/installation/windows/):\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150464795032694875/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
