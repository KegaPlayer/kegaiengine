{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KegaPlayer/kegaiengine/blob/main/Koboldcpp_Colab_(Another_Edited_Edition).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this one: https://colab.research.google.com/github/kalomaze/koboldcpp/blob/alternate_colab/Koboldcpp_Colab_(Improved_Edition).ipynb\n",
        "\n",
        "Original Koboldcpp Colab: https://colab.research.google.com/github/LostRuins/koboldcpp/blob/concedo/colab.ipynb\n",
        "\n",
        "The original Colab is still very barebones, and Kalomaze's original seems to be starting to gather cobwebs. I may still go onwards on keeping this thing for my own amusement, but also try to update it for futureproofing...\n",
        "<sub>(Message from 22/10/2023)</sub>\n",
        "\n",
        "---\n",
        "*Edited koboldcpp colab notebook for the KegAIEngine project.* Experiment to your heart's content, just as I may have done with this before this 'public' release.\n",
        "\n",
        "*   (As of 22/10/2023) Includes installing by the program's official Colab\n",
        "*   It can display the model's name inside the UI when choosing from the dropdown.\n",
        "*   Allows to set up a Kobold Horde worker if you feel like sharing Google's GPU compute power to the world\n",
        "*   A new and expanded selection of models to choose from. The models available are curated under the maker's discretion or if you hit me to add something...\n",
        "*   (Reintroduces?) Ability to keep track of the models used by the user on a (most probably) private spreadsheet\n",
        "\n",
        "## Credits\n",
        "- Made with ~~spite~~ love by kalomaze ❤️ <sub>\n",
        " - (also here's the part where I (kalomaze) shill my [Patreon](https://www.patreon.com/kalomaze) if you care!)</sub>\n",
        "- Edited and (sorta) updated with a bit of spite by KegaPlayer. Not owner of a Patreon but can be contacted at:\n",
        " * Discord: [kegaplayer](https://lookup.guru/183692020131299339)\n",
        " * Twitter: [KegaMPlayer](https://twitter.com/KegaMPlayer)\n",
        " * Mail: <potatokaigen@gmail.com>\n",
        "\n",
        "### Koboldcpp is not a software originally made by te aforementioned. This is just to make it easy to use on Colab, for research use and beyond. You can find the original GitHub repository for it here: https://github.com/LostRuins/koboldcpp"
      ],
      "metadata": {
        "id": "VhbuN9Yf6bc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "outputs": [],
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
        {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from google.oauth2.service_account import Credentials\n",
        "import hashlib\n",
        "import gspread\n",
        "\n",
        "#@title # **Koboldcpp Colab (Another Edited Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "#Koboldcpp now can autorun pretty much autorun itself without building, yay!\n",
        "\n",
        "Model_C = \"Emerhyst-20B-GGUF\" #@param [\"Athena-v3-GGUF\", \"Emerhyst-20B-GGUF\", \"MLewdBoros-L2-13B-GGUF\", \"Mythalion-13B-GGUF\", \"MythoMax-L2-13B-GGUF\", \"Pygmalion-2-13B\", \"ReMM-v2-L2-13B-GGUF\", \"ReMM-SLERP-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\", \"Xwin-MLewd-13B-v0.2-GGUF\", \"13B-Thorns-L2-GGUF\"]\n",
        "Model = Model_C\n",
        "Quant_Method = \"3_K_S\" #@param [\"3_K_S\", \"3_K_L\", \"4_K_S\", \"4_K_M\", \"5_K_S\", \"5_K_M\"]\n",
        "\n",
        "#@markdown #### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### OPTIONAL: Use LoRA\n",
        "Use_Lora = False #@param {type:\"boolean\"}\n",
        "Lora_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Launch Options\n",
        "\n",
        "Layers = \"65\" # @param [\"32\", \"43\", \"65\"] {allow-input: true}\n",
        "#@markdown (32 are all layers for 7B models (if you do load one). 43 are all layers for 13B models. 65 are all layers for 20B models)\n",
        "Context = \"4096\" #@param [\"512\",\"1024\",\"2048\",\"3072\",\"4096\",\"6144\",\"8192\",\"12288\",\"16384\"]{allow-input: true}\n",
        "Smart_Context = False #@param {type:\"boolean\"}\n",
        "#@markdown (The default (and recommended) configuration is 4096. Use higher sizes with caution, lower sizes are NOT recommended. Use Smart Context at your own risk.)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ##### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "#@markdown (Possibly broken...)\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# @markdown # Setup Horde Worker\n",
        "\n",
        "# @markdown (Available for experimental gimmick reasons. The maker may not be held responsible if you end up banned off the service from using this)\n",
        "enable_horde_worker = False #@param {type:\"boolean\"}\n",
        "hordemodelname = \"\" #@param{type:\"string\"}\n",
        "hordegenlength = \"\" #@param{type:\"string\"}\n",
        "hordeapikey = \"\" #@param{type:\"string\"}\n",
        "hordeworkername = \"KegAIEngine\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Analytics\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "# Updates the spreadsheet with the stats of the model when ran\n",
        "def update_llama_stats(DownloadedModel_path):\n",
        "    # Initialize gspread\n",
        "    scope = [\n",
        "        'https://www.googleapis.com/auth/spreadsheets',\n",
        "        'https://www.googleapis.com/auth/drive.file',\n",
        "        'https://www.googleapis.com/auth/drive'\n",
        "    ]\n",
        "\n",
        "    os.makedirs(\"/content/koboldcpp/stats/\", exist_ok=True)\n",
        "    !wget -q https://cdn.discordapp.com/attachments/945486970883285045/1114717554481569802/peppy-generator-388800-07722f17a188.json -O /content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json\n",
        "    config_path = '/content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json'\n",
        "\n",
        "    if os.path.exists(config_path):\n",
        "        # File exists, proceed with creation of creds and client\n",
        "        creds = Credentials.from_service_account_file(config_path, scopes=scope)\n",
        "        client = gspread.authorize(creds)\n",
        "    else:\n",
        "        # File does not exist, print message and skip creation of creds and client\n",
        "        print(\"Sheet credential file missing.\")\n",
        "        exit()  # Exit the script if the credentials are missing\n",
        "\n",
        "    # Open the Google Sheet\n",
        "    book = client.open(\"LlamaStats\")\n",
        "    sheet = book.get_worksheet(0)  # get the first sheet\n",
        "\n",
        "    DownloadedModel_name = os.path.basename(DownloadedModel_path)\n",
        "    DownloadedModel_hash = calculate_md5(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "    colA_values = sheet.col_values(1)\n",
        "    colB_values = sheet.col_values(2)\n",
        "    colC_values = sheet.col_values(3)\n",
        "\n",
        "    update_idx = -1\n",
        "\n",
        "    for idx in range(len(colA_values)):\n",
        "        if colA_values[idx] == DownloadedModel_name and idx < len(colB_values) and colB_values[idx] == DownloadedModel_hash:\n",
        "            update_idx = idx + 1\n",
        "            break\n",
        "\n",
        "    if update_idx == -1:\n",
        "        update_idx = len(colA_values) + 1\n",
        "\n",
        "    current_count = colC_values[update_idx - 1] if update_idx <= len(colC_values) else ''\n",
        "    if current_count.isdigit():\n",
        "        new_count = str(int(current_count) + 1)\n",
        "    else:\n",
        "        new_count = '1'\n",
        "\n",
        "    # Batch update to Google Sheets\n",
        "    cell_list = [\n",
        "        gspread.models.Cell(update_idx, 1, DownloadedModel_name),\n",
        "        gspread.models.Cell(update_idx, 2, DownloadedModel_hash),\n",
        "        gspread.models.Cell(update_idx, 3, new_count),\n",
        "        gspread.models.Cell(update_idx, 4, DownloadedModel_path)\n",
        "    ]\n",
        "    sheet.update_cells(cell_list)\n",
        "    print(\"\\nUpdating values...\\n\")\n",
        "\n",
        "#@markdown ##### OPTIONAL: Submit Download stats (for measuring model usage/popularity)\n",
        "Submit_Download_Stats = False #@param {type:\"boolean\"}\n",
        "\n",
        "#For horde\n",
        "horde_params = set()\n",
        "if enable_horde_worker:\n",
        "  horde_params.add(hordegenlength)\n",
        "  horde_params.add(Context)\n",
        "  if Use_Manual_Model:\n",
        "    if hordemodelname.strip():\n",
        "      print(f\"\\nManual Model detected; Using the name provided by the user: {hordemodelname}\")\n",
        "    else:\n",
        "      print(f\"\\nManual Model detected but model name not provided, falling back to default name used\\n\")\n",
        "      hordemodelname = \"concedo\"\n",
        "\n",
        "model_links = {\n",
        "    \"Athena-v3-GGUF\": \"https://huggingface.co/TheBloke/Athena-v3-GGUF/resolve/main/athena-v3.Q{}.gguf\",\n",
        "    \"Emerhyst-20B-GGUF\": \"https://huggingface.co/TheBloke/Emerhyst-20B-GGUF/resolve/main/emerhyst-20b.Q{}.gguf\",\n",
        "    \"MLewdBoros-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MLewdBoros-L2-13B-GGUF/resolve/main/mlewdboros-l2-13b.Q{}.gguf\",\n",
        "    \"Mythalion-13B-GGUF\": \"https://huggingface.co/TheBloke/Mythalion-13B-GGUF/resolve/main/mythalion-13b.Q{}.gguf\",\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"Pygmalion-2-13B\": \"https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q{}.gguf\",\n",
        "    \"ReMM-v2-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-v2-L2-13B-GGUF/resolve/main/remm-v2-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-SLERP-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\",\n",
        "    \"Xwin-MLewd-13B-v0.2-GGUF\": \"https://huggingface.co/TheBloke/Xwin-MLewd-13B-v0.2-GGUF/blob/main/xwin-mlewd-13b-v0.2.Q{}.gguf\",\n",
        "    \"13B-Thorns-L2-GGUF\": \"https://huggingface.co/TheBloke/13B-Thorns-L2-GGUF/blob/main/13b-thorns-l2.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    # Use aria2c to download\n",
        "    print(\"Installing/updating aria2c...\")\n",
        "    !apt-get install aria2 -y >/dev/null 2>&1\n",
        "    print(\"Finished installing aria2c.\")\n",
        "\n",
        "    os.makedirs('/content/koboldcpp/', exist_ok=True)\n",
        "    if Use_Lora:\n",
        "      if Lora_Link.strip():\n",
        "          # Lora is enabled & link provided\n",
        "          print(\"\\nLora detected, will apply to model.\\n\")\n",
        "          lora = Lora_Link.replace('/blob/', '/resolve/')\n",
        "      else:\n",
        "          # Lora is enabled but no link\n",
        "          print(\"\\nWarning: Lora enabled, but no link, not applying.\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Manual_Link.strip():\n",
        "            # Manual Model is enabled, and a link is provided\n",
        "            print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "            Model = Manual_Link.replace('/blob/', '/resolve/')\n",
        "        else:\n",
        "            # Manual Model is enabled, but no link is provided\n",
        "            print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "            if Model in model_links:\n",
        "                Model = model_links[Model].format(Quant_Method)\n",
        "    else:\n",
        "        # Model is in model_links and has a supported format\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "    if not re.search(r'(\\.gguf|\\.ggml|\\.bin|\\.safetensors)$', Model):\n",
        "        print(\"--------------------------\\n5 SECOND WARNING: Manual link provided doesn't end with a supported format.\\nAre you sure you provided a direct link?\\n--------------------------\\n\")\n",
        "        time.sleep(5)\n",
        "    elif Model.startswith('https://huggingface.co/') and not re.search(r'^https://huggingface\\.co/.+/.+/.+/.+/[^/]+\\.[^/]+$', Model):\n",
        "        print(\"--------------------------\\n10 SECOND WARNING: The HuggingFace link provided is of the entire model repository.\\nPlease find the direct link to the quant you want to use.\\n--------------------------\\n\")\n",
        "        time.sleep(10)\n",
        "\n",
        "def download_model_and_lora():\n",
        "    if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n--------------------------\\nDownloading {os.path.basename(Model)}...\")\n",
        "        os.chdir(\"/content/koboldcpp\")\n",
        "        !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        elapsed_time = time.time() - start_time # Calculate and display elapsed time\n",
        "        print(f\"\\nDownload took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        if Use_Lora:\n",
        "          print(f\"\\n--------------------------\\nDownloading {os.path.basename(lora)}...\")\n",
        "          os.chdir(\"/content/koboldcpp\")\n",
        "          !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o lora.bin 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/model.gguf') and os.path.getsize(\"/content/koboldcpp/model.gguf\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/lora.bin') and os.path.getsize(\"/content/koboldcpp/lora.bin\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/lora.bin\")\n",
        "\n",
        "        if Submit_Download_Stats and os.path.exists(\"/content/koboldcpp/model.gguf\"):\n",
        "            DownloadedModel = Model[:]  # DownloadedModel is used for download stats\n",
        "            update_llama_stats(DownloadedModel)\n",
        "\n",
        "        print(\"--------------------------\\n\")\n",
        "    else:\n",
        "         print(\"--------------------------\\nModel already downloaded; skipping redownload.\\nDisconnect and delete runtime if you need to restart the colab fully.\\n--------------------------\\n\")\n",
        "\n",
        "thread = threading.Thread(target=download_model_and_lora)\n",
        "\n",
        "# Checking if you already have a Kobold install\n",
        "if not os.path.exists(\"/content/koboldcpp/llama.cpp\"):\n",
        "    print(\"Downloading & extracting prebuilt Koboldcpp...\")\n",
        "\n",
        "    thread.start()\n",
        "\n",
        "    %cd /content\n",
        "    !git clone https://github.com/LostRuins/koboldcpp\n",
        "    %cd /content/koboldcpp\n",
        "    kvers = !(cat koboldcpp.py | grep 'KcppVersion = ' | cut -d '\"' -f2)\n",
        "    kvers = kvers[0]\n",
        "    !echo Finding prebuilt binary for {kvers}\n",
        "    !wget -c https://huggingface.co/concedo/koboldcpp/resolve/main/prebuilt_binaries/{kvers}.so\n",
        "    !test -f {kvers}.so && mv {kvers}.so koboldcpp_cublas.so || echo Prebuilt Binary Does Not Exist\n",
        "    !test -f koboldcpp_cublas.so && echo Prebuilt Binary Exists || make koboldcpp_cublas LLAMA_CUBLAS=1\n",
        "    !cp koboldcpp_cublas.so koboldcpp_cublas.dat\n",
        "\n",
        "    print(\"\\nKobold extraction to /content/koboldcpp/ completed!\")\n",
        "    print(\"--------------------------\\n\")\n",
        "else:\n",
        "    # In case koboldcpp already exists, just start the model download\n",
        "    thread.start()\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "if not os.path.exists(\"/content/koboldcpp/cloudflared-linux-amd64\"):\n",
        "    os.chdir(\"/content/koboldcpp\")\n",
        "    print(\"\\n--------------------------\\nDownloading cloudflared...\\n\")\n",
        "    !wget -c -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "print(\"Attempting to launch cloudflared server...\")\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "\n",
        "# Check nohup.out for \"protocol=quic\" which signifies it launched\n",
        "print(\"Checking if the server is up...\\n\")\n",
        "while True:\n",
        "    time.sleep(1)\n",
        "    with open('nohup.out', 'r') as f:\n",
        "        if 'connIndex=' in f.read():\n",
        "            print(\"--------------------------\\nServer up!\")\n",
        "            break\n",
        "\n",
        "!cat nohup.out\n",
        "print(\"--------------------------\\n\")\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "thread.join()\n",
        "\n",
        "def hordeworkerenabled():\n",
        "  if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername ---onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "  elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername ---onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "  else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n",
        "\n",
        "def runkbcpp():\n",
        "  if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Smart_Context:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "        else:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "    else:\n",
        "        if Smart_Context:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "        else:\n",
        "          !python koboldcpp.py model.gguf --lora lora.bin --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "\n",
        "  elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --usecublas 0 mmq --multiuser --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C 1 1 --onready \"echo Connect to the link below && cat nohup.out | grep trycloudflare.com\"\n",
        "  else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n",
        "\n",
        "if enable_horde_worker:\n",
        "  hordeworkerenabled()\n",
        "else:\n",
        "  runkbcpp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQl3v9wali-"
      },
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "<sub>Note that some of the images may be outdated. But the overall function of the Colab is still the same!</sub>\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in [SillyTavern](https://docs.sillytavern.app/installation/windows/):\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150464795032694875/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
