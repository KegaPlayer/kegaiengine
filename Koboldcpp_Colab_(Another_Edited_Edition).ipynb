{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KegaPlayer/kegaiengine/blob/main/Koboldcpp_Colab_(Another_Edited_Edition).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this one: https://colab.research.google.com/github/kalomaze/koboldcpp/blob/alternate_colab/Koboldcpp_Colab_(Improved_Edition).ipynb\n",
        "\n",
        "**Kalomaze, you fucking rock.** Though I may still come and commit in my own dumb changes...I have my reasons, really\n",
        "\n",
        "---\n",
        "*Edited koboldcpp colab notebook for the KegAIEngine project.* Experiment to your heart's content, just as I may have done with this before this 'public' release.\n",
        "\n",
        "*   It can display the model's name inside the UI when choosing from the dropdown.\n",
        "*   Removes the usage data function because I honestly don't give a flying fudgy about what you use to RP and I want to at least not copy everything from the original 1:1 and do honestly minor changes.\n",
        "*   Allows to set up a Kobold Horde worker if you feel like sharing Google's GPU compute power to the world\n",
        "*   A selection of models by yours truly. The models available are curated under the maker's discretion or if you hit me to add something...\n",
        "\n",
        "Oh yeah, you can find me at Discord: [kegaplayer](https://lookup.guru/183692020131299339) or at Twitter: [KegaMPlayer](https://twitter.com/KegaMPlayer) if you wanna chat or scream at me for being a hack at Python. I'm trying to learn still."
      ],
      "metadata": {
        "id": "VhbuN9Yf6bc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "outputs": [],
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from google.oauth2.service_account import Credentials\n",
        "import hashlib\n",
        "import gspread\n",
        "\n",
        "#@title # **Koboldcpp 1.44 Colab (Improved Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "# URL of the built koboldcpp folder\n",
        "url = \"https://huggingface.co/kalomaze/ColabDependencies/resolve/main/koboldcpp.tar.gz\"\n",
        "\n",
        "Model_C = \"Emerhyst-20B-GGUF\" #@param [\"Athena-v3-GGUF\", \"Emerhyst-20B-GGUF\", \"MLewdBoros-L2-13B-GGUF\", \"Mythalion-13B-GGUF\", \"MythoMax-L2-13B-GGUF\", \"Pygmalion-2-13B\", \"ReMM-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\"]\n",
        "Model = Model_C\n",
        "Quant_Method = \"3_K_S\" #@param [\"3_K_S\", \"3_K_L\", \"4_K_S\", \"4_K_M\", \"5_K_S\", \"5_K_M\"]\n",
        "\n",
        "#@markdown #### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### OPTIONAL: Use LoRA\n",
        "Use_Lora = False #@param {type:\"boolean\"}\n",
        "Lora_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Launch Options\n",
        "\n",
        "Layers = \"65\" # @param [\"43\", \"65\"] {allow-input: true}\n",
        "Context = \"4096\" #@param [4096]{allow-input: true}\n",
        "Smart_Context = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ##### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# @markdown # Setup Horde Worker\n",
        "\n",
        "# @markdown (Kinda just up for gimmick reasons. I don't make myself responsible if you end up banned from using this)\n",
        "enable_horde_worker = False #@param {type:\"boolean\"}\n",
        "hordemodelname = \"\" #@param{type:\"string\"}\n",
        "hordegenlength = \"\" #@param{type:\"string\"}\n",
        "hordeapikey = \"\" #@param{type:\"string\"}\n",
        "hordeworkername = \"KegAIEngine\" #@param{type:\"string\"}\n",
        "#For horde\n",
        "horde_params = set()\n",
        "if enable_horde_worker:\n",
        "  horde_params.add(hordegenlength)\n",
        "  horde_params.add(Context)\n",
        "\n",
        "  if Use_Manual_Model:\n","
        "   if hordemodelname.strip():\n",
        "    print(f\"\\nManual Model detected; Using the name provided: $hordemodelname\\n\")\n",
        "   else\n",
        "    print(f\"\\nManual Model detected but model name not provided, falling back to default option\\n\")\n",
        "    hordemodelname = \"concedo\"\n",
        "\n",
        "model_links = {\n",
        "    \"Athena-v3-GGUF\": \"https://huggingface.co/TheBloke/Athena-v3-GGUF/resolve/main/athena-v3.Q{}.gguf\",\n",
        "    \"Emerhyst-20B-GGUF\": \"https://huggingface.co/TheBloke/Emerhyst-20B-GGUF/resolve/main/emerhyst-20b.Q{}.gguf\",\n",
        "    \"MLewdBoros-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MLewdBoros-L2-13B-GGUF/resolve/main/mlewdboros-l2-13b.Q{}.gguf\",\n",
        "    \"Mythalion-13B-GGUF\": \"https://huggingface.co/TheBloke/Mythalion-13B-GGUF/resolve/main/mythalion-13b.Q{}.gguf\",\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"Pygmalion-2-13B\": \"https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q{}.gguf\",\n",
        "    \"ReMM-L2-13B-GGUF\": \"https://huggingface.co/Undi95/ReMM-v2.2-L2-13B-GGUF/resolve/main/ReMM-v2.2-L2-13B.q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    # Use aria2c to download\n",
        "    print(\"Installing/updating aria2c...\")\n",
        "    !apt-get install aria2 -y >/dev/null 2>&1\n",
        "    print(\"Finished installing aria2c.\")\n",
        "\n",
        "    os.makedirs('/content/koboldcpp/', exist_ok=True)\n",
        "    if Use_Lora:\n",
        "      if Lora_Link.strip():\n",
        "          # Lora is enabled & link provided\n",
        "          print(\"\\nLora detected, will apply to model.\\n\")\n",
        "          lora = Lora_Link.replace('/blob/', '/resolve/')\n",
        "      else:\n",
        "          # Lora is enabled but no link\n",
        "          print(\"\\nWarning: Lora enabled, but no link, not applying.\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Manual_Link.strip():\n",
        "            # Manual Model is enabled, and a link is provided\n",
        "            print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "            Model = Manual_Link.replace('/blob/', '/resolve/')\n",
        "        else:\n",
        "            # Manual Model is enabled, but no link is provided\n",
        "            print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "            if Model in model_links:\n",
        "                Model = model_links[Model].format(Quant_Method)\n",
        "    else:\n",
        "        # Model is in model_links and has a supported format\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "    if not re.search(r'(\\.gguf|\\.ggml|\\.bin|\\.safetensors)$', Model):\n",
        "        print(\"--------------------------\\n5 SECOND WARNING: Manual link provided doesn't end with a supported format.\\nAre you sure you provided a direct link?\\n--------------------------\\n\")\n",
        "        time.sleep(5)\n",
        "    elif Model.startswith('https://huggingface.co/') and not re.search(r'^https://huggingface\\.co/.+/.+/.+/.+/[^/]+\\.[^/]+$', Model):\n",
        "        print(\"--------------------------\\n10 SECOND WARNING: The HuggingFace link provided is of the entire model repository.\\nPlease find the direct link to the quant you want to use.\\n--------------------------\\n\")\n",
        "        time.sleep(10)\n",
        "\n",
        "def download_model_and_lora():\n",
        "    if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n--------------------------\\nDownloading {os.path.basename(Model)}...\")\n",
        "        os.chdir(\"/content/koboldcpp\")\n",
        "        !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        elapsed_time = time.time() - start_time # Calculate and display elapsed time\n",
        "        print(f\"\\nDownload took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        if Use_Lora:\n",
        "          print(f\"\\n--------------------------\\nDownloading {os.path.basename(lora)}...\")\n",
        "          os.chdir(\"/content/koboldcpp\")\n",
        "          !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $Model -d /content/koboldcpp -o lora.bin 2>&1 | grep -Ev 'Redirecting'\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/model.gguf') and os.path.getsize(\"/content/koboldcpp/model.gguf\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/lora.bin') and os.path.getsize(\"/content/koboldcpp/lora.bin\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/lora.bin\")\n",
        "\n",
        "        print(\"--------------------------\\n\")\n",
        "    else:\n",
        "         print(\"--------------------------\\nModel already downloaded; skipping redownload.\\nDisconnect and delete runtime if you need to restart the colab fully.\\n--------------------------\\n\")\n",
        "\n",
        "thread = threading.Thread(target=download_model_and_lora)\n",
        "\n",
        "# Checking if you already have a Kobold install\n",
        "if not os.path.exists(\"/content/koboldcpp/llama.cpp\"):\n",
        "    if not Force_Update_Build:\n",
        "        print(\"--------------------------\")\n",
        "        print(\"Force_Update_Build is set to False. Proceeding...\")\n",
        "        print(\"Downloading & extracting prebuilt Koboldcpp 1.44...\")\n",
        "\n",
        "        thread.start()\n",
        "\n",
        "        # Downloading file\n",
        "        print(f\"Starting download from: {url}\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        with open(filename, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"Downloaded file: {filename}\")\n",
        "\n",
        "        # Create the koboldcpp directory if it doesn't exist\n",
        "        destination_path = '/content/koboldcpp/'\n",
        "        if not os.path.exists(destination_path):\n",
        "            os.makedirs(destination_path)\n",
        "\n",
        "        # Extracting the .tar.gz archive\n",
        "        with tarfile.open(filename, 'r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                adjusted_path = os.path.join(destination_path, member.name)\n",
        "                try:\n",
        "                    tar.extract(member, path=destination_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting to '{adjusted_path}': {str(e)}\")\n",
        "\n",
        "        print(\"\\nKobold extraction to /content/koboldcpp/ completed!\")\n",
        "        print(\"--------------------------\\n\")\n",
        "    else:\n",
        "        print(\"--------------------------\\nSkipping prebuilt kobold, will build manually...\")\n",
        "        thread.start()\n",
        "        !git clone https://github.com/LostRuins/koboldcpp\n",
        "        %cd /content/koboldcpp\n",
        "        !make LLAMA_CUBLAS=1\n",
        "        print(\"--------------------------\")\n",
        "else:\n",
        "    # In case koboldcpp already exists, just start the model download\n",
        "    thread.start()\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "if not os.path.exists(\"/content/koboldcpp/cloudflared-linux-amd64\"):\n",
        "    os.chdir(\"/content/koboldcpp\")\n",
        "    print(\"\\n--------------------------\\nDownloading cloudflared...\\n\")\n",
        "    !wget -c -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "print(\"Attempting to launch cloudflared server...\")\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "\n",
        "# Check nohup.out for \"protocol=quic\" which signifies it launched\n",
        "print(\"Checking if the server is up...\\n\")\n",
        "while True:\n",
        "    time.sleep(1)\n",
        "    with open('nohup.out', 'r') as f:\n",
        "        if 'connIndex=' in f.read():\n",
        "            print(\"--------------------------\\nServer up!\")\n",
        "            break\n",
        "\n",
        "!cat nohup.out\n",
        "print(\"--------------------------\\n\")\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "thread.join()\n",
        "\n",
        "if os.path.exists(\"/content/koboldcpp/callback_url.py\"):\n",
        "    os.remove(\"/content/koboldcpp/callback_url.py\")\n",
        "\n",
        "def hordeworkerenabled():\n",
        "  if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"/content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"/content/koboldcpp/callback_url.py\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"/content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"/content/koboldcpp/callback_url.py\"\n",
        "  elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $hordemodelname {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "    else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C {' '.join(horde_params)} $hordeapikey $hordeworkername --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "  else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n",
        "\n",
        "if enable_horde_worker:\n",
        "  hordeworkerenabled()\n",
        "elif os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "  !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "  print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "  print(\"--------------------------\\n\")\n",
        "  if Use_Manual_Model:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo --onready \"/content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo --onready \"/content/koboldcpp/callback_url.py\"\n",
        "  else:\n",
        "      if Smart_Context:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C --onready \"/content/koboldcpp/callback_url.py\"\n",
        "      else:\n",
        "        !python koboldcpp.py model.gguf --lora lora.bin --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C --onready \"/content/koboldcpp/callback_url.py\"\n",
        "elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "  !wget -q https://github.com/kalomaze/koboldcpp/raw/alternate_colab/callback_url.py\n",
        "  print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "  print(\"--------------------------\\n\")\n",
        "  if Use_Manual_Model:\n",
        "    if Smart_Context:\n",
        "      !python koboldcpp.py model.gguf --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "    else:\n",
        "      !python koboldcpp.py model.gguf --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "  else:\n",
        "    if Smart_Context:\n",
        "      !python koboldcpp.py model.gguf --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "    else:\n",
        "      !python koboldcpp.py model.gguf --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig $Model_C --onready \"python /content/koboldcpp/callback_url.py\"\n",
        "else:\n",
        "  print(\"Failed to download the GGUF model or LoRA. Please retry.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQl3v9wali-"
      },
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in [SillyTavern](https://docs.sillytavern.app/installation/windows/):\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150464795032694875/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)\n",
        "\n",
        "---\n",
        "\n",
        "# Credits\n",
        "### - Made with ~~spite~~ love by kalomaze ❤️ <sub>(also here's the part where I shill my [Patreon](https://www.patreon.com/kalomaze) if you care!)</sub>\n",
        "### - Koboldcpp is not my software, this is just to make it easy to use on Colab, for research use and beyond. You can find the original GitHub repository for it here: https://github.com/LostRuins/koboldcpp"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
